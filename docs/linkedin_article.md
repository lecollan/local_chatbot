# ü§ñ Crea tu propio Chatbot Local con IA en minutos: Python + Ollama + Gemma

¬øTe preocupa la privacidad de tus datos al usar IA? ¬øO simplemente quieres tener un asistente inteligente que funcione 100% offline en tu port√°til?

Recientemente he estado experimentando con la ejecuci√≥n de **LLMs (Large Language Models)** en local y los resultados son sorprendentes. En este art√≠culo, quiero compartir c√≥mo puedes montar tu propio chatbot utilizando **Gemma** (el modelo abierto de Google), **Ollama** y **Streamlit**.

## ¬øPor qu√© en Local?

M√°s all√° del "hype", correr modelos en tu propia infraestructura (o port√°til) tiene ventajas claras:
- **Privacidad Total**: Tus datos nunca salen de tu m√°quina. Ideal para consultor√≠a o manejo de informaci√≥n sensible.
- **Coste Cero**: No hay facturas de API por token.
- **Latencia**: Respuesta inmediata sin depender de tu conexi√≥n a internet.

## El Stack Tecnol√≥gico

Para este proyecto he utilizado:
1.  **[Ollama](https://ollama.com)**: La herramienta definitiva para gestionar y ejecutar LLMs en local (Windows/Mac/Linux).
2.  **[Gemma:2b](https://blog.google/technology/developers/gemma-open-models/)**: La versi√≥n ligera del modelo de Google, optimizada para correr en hardware de consumo.
3.  **Python + Streamlit**: Para crear la interfaz de chat en menos de 50 l√≠neas de c√≥digo.

## Paso a Paso

### 1. Preparando el Motor: Ollama
Lo primero es instalar Ollama y descargar el modelo. Desde la terminal:

```bash
ollama pull gemma:2b
```

Este comando descarga el modelo (unos 1.7GB) y lo deja listo para usar.

### 2. El C√≥digo (Python)

La magia ocurre en un script de Python muy sencillo. Usamos la librer√≠a oficial de `ollama` para Python y `streamlit` para la UI.

Aqu√≠ tienes la estructura b√°sica de `chatbot_app.py`:

```python
import streamlit as st
import ollama

st.title("ü§ñ Gemma Local Chatbot")

# Inicializar historial
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# Mostrar mensajes anteriores
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Capturar input del usuario
if prompt := st.chat_input("Escribe tu mensaje..."):
    # Guardar y mostrar mensaje usuario
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generar respuesta
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # Llamada a Ollama en modo stream
        stream = ollama.chat(
            model='gemma:2b',
            messages=st.session_state['messages'],
            stream=True,
        )
        
        for chunk in stream:
            content = chunk['message']['content']
            if content:
                full_response += content
                message_placeholder.markdown(full_response + "‚ñå")
                
        message_placeholder.markdown(full_response)
    
    st.session_state["messages"].append({"role": "assistant", "content": full_response})
```

### 3. ¬°A funcionar!

Con una simple l√≠nea en la terminal, tu chatbot cobra vida:

```bash
streamlit run chatbot_app.py
```

## Resultados

Usando el modelo `gemma:2b`, la velocidad es impresionante incluso en equipos sin una GPU dedicada potente. La capacidad de razonamiento es suficiente para tareas de asistencia, resumen y generaci√≥n de texto creativa.

Este peque√±o proyecto demuestra que la **IA Soberana** es accesible hoy para cualquier desarrollador. Ya no dependemos exclusivamente de las grandes APIs para integrar inteligencia en nuestras aplicaciones.

---
¬øHas probado a correr modelos en local? ¬øQu√© uso le dar√≠as en tu entorno profesional? üëá

#IA #LocalLLM #Python #Ollama #Gemma #PrivacidadDeDatos #DevCommunity
